{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14640905,"sourceType":"datasetVersion","datasetId":9352726}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.067359Z","iopub.execute_input":"2026-01-28T08:13:10.067635Z","iopub.status.idle":"2026-01-28T08:13:10.076585Z","shell.execute_reply.started":"2026-01-28T08:13:10.067607Z","shell.execute_reply":"2026-01-28T08:13:10.075994Z"}},"outputs":[{"name":"stdout","text":"['depressionintamil']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\ndef print_tree(start_path, indent=\"\"):\n    for item in sorted(os.listdir(start_path)):\n        path = os.path.join(start_path, item)\n        print(indent + \"|-- \" + item)\n        if os.path.isdir(path):\n            print_tree(path, indent + \"    \")\n\nDATASET_PATH = \"/kaggle/input/depressionintamil\"  # change if needed\n#print_tree(DATASET_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.078958Z","iopub.execute_input":"2026-01-28T08:13:10.079600Z","iopub.status.idle":"2026-01-28T08:13:10.092537Z","shell.execute_reply.started":"2026-01-28T08:13:10.079578Z","shell.execute_reply":"2026-01-28T08:13:10.091756Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"BASE_PATH = \"/kaggle/input/depressionintamil\"\nimport os\nfrom glob import glob\n\nTAMIL_TRAIN_PATH = os.path.join(BASE_PATH, \"Tamil\")\nTEST_PATH = os.path.join(BASE_PATH, \"Test-set-tamil\", \"Test-set-tamil\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.093346Z","iopub.execute_input":"2026-01-28T08:13:10.093598Z","iopub.status.idle":"2026-01-28T08:13:10.107043Z","shell.execute_reply.started":"2026-01-28T08:13:10.093573Z","shell.execute_reply":"2026-01-28T08:13:10.106432Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_files = []\ntrain_labels = []\n\n# Depressed\ndepressed_files = glob(\n    os.path.join(TAMIL_TRAIN_PATH, \"Depressed\", \"Train_set\", \"*.wav\")\n)\ntrain_files.extend(depressed_files)\ntrain_labels.extend([1] * len(depressed_files))\n\n# Non-depressed\nnon_depressed_files = glob(\n    os.path.join(TAMIL_TRAIN_PATH, \"Non-depressed\", \"Train_set\", \"*.wav\")\n)\ntrain_files.extend(non_depressed_files)\ntrain_labels.extend([0] * len(non_depressed_files))\n\nprint(\"Training samples:\", len(train_files))\nprint(\"Depressed samples:\", len(depressed_files))\nprint(\"Non-depressed samples:\", len(non_depressed_files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.107997Z","iopub.execute_input":"2026-01-28T08:13:10.108263Z","iopub.status.idle":"2026-01-28T08:13:10.168396Z","shell.execute_reply.started":"2026-01-28T08:13:10.108236Z","shell.execute_reply":"2026-01-28T08:13:10.167749Z"}},"outputs":[{"name":"stdout","text":"Training samples: 1374\nDepressed samples: 454\nNon-depressed samples: 920\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"test_files = sorted(glob(os.path.join(TEST_PATH, \"*.wav\")))\n\nprint(\"Test samples:\", len(test_files))\nprint(\"Example test file:\", test_files[0])\nprint(\"\\nSample training files:\")\nfor i in range(3):\n    print(train_files[i], \"→ label:\", train_labels[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.169256Z","iopub.execute_input":"2026-01-28T08:13:10.169515Z","iopub.status.idle":"2026-01-28T08:13:10.197183Z","shell.execute_reply.started":"2026-01-28T08:13:10.169486Z","shell.execute_reply":"2026-01-28T08:13:10.196381Z"}},"outputs":[{"name":"stdout","text":"Test samples: 160\nExample test file: /kaggle/input/depressionintamil/Test-set-tamil/Test-set-tamil/t1.wav\n\nSample training files:\n/kaggle/input/depressionintamil/Tamil/Depressed/Train_set/D_S00_22-2.wav → label: 1\n/kaggle/input/depressionintamil/Tamil/Depressed/Train_set/D_S00_10-4.wav → label: 1\n/kaggle/input/depressionintamil/Tamil/Depressed/Train_set/D_S00_38-2.wav → label: 1\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.DataFrame({\n    \"path\": train_files,\n    \"label\": train_labels\n})\n\ntest_df = pd.DataFrame({\n    \"path\": test_files\n})\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:10.198192Z","iopub.execute_input":"2026-01-28T08:13:10.198499Z","iopub.status.idle":"2026-01-28T08:13:11.348625Z","shell.execute_reply.started":"2026-01-28T08:13:10.198469Z","shell.execute_reply":"2026-01-28T08:13:11.347870Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0  /kaggle/input/depressionintamil/Tamil/Depresse...      1\n1  /kaggle/input/depressionintamil/Tamil/Depresse...      1\n2  /kaggle/input/depressionintamil/Tamil/Depresse...      1\n3  /kaggle/input/depressionintamil/Tamil/Depresse...      1\n4  /kaggle/input/depressionintamil/Tamil/Depresse...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/depressionintamil/Tamil/Depresse...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/depressionintamil/Tamil/Depresse...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/depressionintamil/Tamil/Depresse...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/depressionintamil/Tamil/Depresse...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/depressionintamil/Tamil/Depresse...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"**First Model**","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport librosa\nfrom transformers import (\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2Model\n)\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom collections import Counter\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:11.350957Z","iopub.execute_input":"2026-01-28T08:13:11.351608Z","iopub.status.idle":"2026-01-28T08:13:38.554506Z","shell.execute_reply.started":"2026-01-28T08:13:11.351573Z","shell.execute_reply":"2026-01-28T08:13:38.553888Z"}},"outputs":[{"name":"stderr","text":"2026-01-28 08:13:25.106295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769588005.301551      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769588005.357874      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769588005.844445      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769588005.844485      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769588005.844488      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769588005.844491      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"DATA_ROOT = \"/kaggle/input/depressionintamil\"\n\nDEPRESSED_DIR = os.path.join(DATA_ROOT, \"Tamil/Depressed/Train_set\")\nNON_DEPRESSED_DIR = os.path.join(DATA_ROOT, \"Tamil/Non-depressed/Train_set\")\n\ntrain_files = []\ntrain_labels = []\n\nfor f in os.listdir(DEPRESSED_DIR):\n    if f.endswith(\".wav\"):\n        train_files.append(os.path.join(DEPRESSED_DIR, f))\n        train_labels.append(1)\n\nfor f in os.listdir(NON_DEPRESSED_DIR):\n    if f.endswith(\".wav\"):\n        train_files.append(os.path.join(NON_DEPRESSED_DIR, f))\n        train_labels.append(0)\n\nprint(\"Total files:\", len(train_files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:38.555292Z","iopub.execute_input":"2026-01-28T08:13:38.555778Z","iopub.status.idle":"2026-01-28T08:13:38.565412Z","shell.execute_reply.started":"2026-01-28T08:13:38.555753Z","shell.execute_reply":"2026-01-28T08:13:38.564769Z"}},"outputs":[{"name":"stdout","text":"Total files: 1374\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def get_speaker_id(path):\n    fname = os.path.basename(path)\n    match = re.search(r'(A\\d+|F\\d+|S\\d+|ND\\d+)', fname)\n    return match.group(1) if match else fname\ndef safe_speaker_split(files, labels, test_size=0.2):\n    groups = [get_speaker_id(f) for f in files]\n\n    for seed in range(100):\n        gss = GroupShuffleSplit(\n            n_splits=1, test_size=test_size, random_state=seed\n        )\n        train_idx, val_idx = next(gss.split(files, labels, groups))\n\n        y_tr = [labels[i] for i in train_idx]\n        y_va = [labels[i] for i in val_idx]\n\n        if len(set(y_tr)) == 2 and len(set(y_va)) == 2:\n            print(f\"✅ Valid split found (seed={seed})\")\n            print(\"Train:\", Counter(y_tr))\n            print(\"Val:\", Counter(y_va))\n            return train_idx, val_idx\n\n    raise RuntimeError(\"❌ Could not find valid speaker split\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:38.566380Z","iopub.execute_input":"2026-01-28T08:13:38.566629Z","iopub.status.idle":"2026-01-28T08:13:38.577109Z","shell.execute_reply.started":"2026-01-28T08:13:38.566595Z","shell.execute_reply":"2026-01-28T08:13:38.576503Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_idx, val_idx = safe_speaker_split(train_files, train_labels)\n\nX_train = [train_files[i] for i in train_idx]\ny_train = [train_labels[i] for i in train_idx]\n\nX_val = [train_files[i] for i in val_idx]\ny_val = [train_labels[i] for i in val_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:38.577965Z","iopub.execute_input":"2026-01-28T08:13:38.578217Z","iopub.status.idle":"2026-01-28T08:13:38.597366Z","shell.execute_reply.started":"2026-01-28T08:13:38.578195Z","shell.execute_reply":"2026-01-28T08:13:38.596666Z"}},"outputs":[{"name":"stdout","text":"✅ Valid split found (seed=0)\nTrain: Counter({0: 736, 1: 423})\nVal: Counter({0: 184, 1: 31})\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n    \"facebook/wav2vec2-xls-r-300m\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:38.598207Z","iopub.execute_input":"2026-01-28T08:13:38.598447Z","iopub.status.idle":"2026-01-28T08:13:39.091121Z","shell.execute_reply.started":"2026-01-28T08:13:38.598414Z","shell.execute_reply":"2026-01-28T08:13:39.090347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de1fa5a4fd84bbd88cc8620c27aea5d"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"class AudioDataset(Dataset):\n    def __init__(self, files, labels):\n        self.files = files\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        audio, _ = librosa.load(self.files[idx], sr=16000)\n\n        return {\n            \"audio\": audio,\n            \"label\": self.labels[idx]\n        }\ndef collate_fn(batch):\n    audios = [item[\"audio\"] for item in batch]\n    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n\n    inputs = feature_extractor(\n        audios,\n        sampling_rate=16000,\n        return_tensors=\"pt\",\n        padding=True\n    )\n\n    return {\n        \"input_values\": inputs.input_values,\n        \"labels\": labels\n    }\ntrain_loader = DataLoader(\n    AudioDataset(X_train, y_train),\n    batch_size=4,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\nval_loader = DataLoader(\n    AudioDataset(X_val, y_val),\n    batch_size=4,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:39.092058Z","iopub.execute_input":"2026-01-28T08:13:39.092331Z","iopub.status.idle":"2026-01-28T08:13:39.099160Z","shell.execute_reply.started":"2026-01-28T08:13:39.092308Z","shell.execute_reply":"2026-01-28T08:13:39.098481Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class XLSRClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Wav2Vec2Model.from_pretrained(\n            \"facebook/wav2vec2-xls-r-300m\"\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 2)\n        )\n\n    def forward(self, input_values):\n        outputs = self.encoder(input_values)\n        pooled = outputs.last_hidden_state.mean(dim=1)\n        return self.classifier(pooled)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:39.099996Z","iopub.execute_input":"2026-01-28T08:13:39.100254Z","iopub.status.idle":"2026-01-28T08:13:39.113481Z","shell.execute_reply.started":"2026-01-28T08:13:39.100233Z","shell.execute_reply":"2026-01-28T08:13:39.112786Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = XLSRClassifier().to(device)\n\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:39.114417Z","iopub.execute_input":"2026-01-28T08:13:39.114660Z","iopub.status.idle":"2026-01-28T08:13:49.404149Z","shell.execute_reply.started":"2026-01-28T08:13:39.114640Z","shell.execute_reply":"2026-01-28T08:13:49.403313Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef69e3f0d0924e29a5b7fca711d3da0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58dbde6958a2434991c7b44b61a04538"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"class_counts = np.bincount(y_train, minlength=2)\n\nweights = torch.tensor(\n    class_counts.sum() / (2 * class_counts),\n    device=device,\n    dtype=torch.float\n)\n\ncriterion = nn.CrossEntropyLoss(weight=weights)\noptimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:49.405193Z","iopub.execute_input":"2026-01-28T08:13:49.406246Z","iopub.status.idle":"2026-01-28T08:13:49.411525Z","shell.execute_reply.started":"2026-01-28T08:13:49.406209Z","shell.execute_reply":"2026-01-28T08:13:49.410785Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\nEPOCHS = 6\n\nfor epoch in range(EPOCHS):\n    # ---------- TRAIN ----------\n    model.train()\n    total_loss = 0\n    train_preds, train_trues = [], []\n\n    for batch in train_loader:\n        optimizer.zero_grad()\n\n        inputs = batch[\"input_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        logits = model(inputs)\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)\n        train_preds.extend(preds.cpu().numpy())\n        train_trues.extend(labels.cpu().numpy())\n\n    train_loss = total_loss / len(train_loader)\n    train_acc = accuracy_score(train_trues, train_preds)\n\n    # ---------- VALIDATION ----------\n    model.eval()\n    val_preds, val_trues = [], []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = batch[\"input_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            logits = model(inputs)\n            preds = torch.argmax(logits, dim=1)\n\n            val_preds.extend(preds.cpu().numpy())\n            val_trues.extend(labels.cpu().numpy())\n\n    val_acc = accuracy_score(val_trues, val_preds)\n    val_f1 = f1_score(val_trues, val_preds, average=\"macro\")\n\n    # ---------- LOG ----------\n    print(\n        f\"Epoch {epoch+1}/{EPOCHS} | \"\n        f\"Train Loss: {train_loss:.4f} | \"\n        f\"Train Acc: {train_acc:.4f} | \"\n        f\"Val Acc: {val_acc:.4f} | \"\n        f\"Val Macro-F1: {val_f1:.4f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:13:49.412389Z","iopub.execute_input":"2026-01-28T08:13:49.412670Z","iopub.status.idle":"2026-01-28T08:27:04.606124Z","shell.execute_reply.started":"2026-01-28T08:13:49.412638Z","shell.execute_reply":"2026-01-28T08:27:04.605326Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755296d8c9e349c990d89708e11f54a2"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/6 | Train Loss: 0.4721 | Train Acc: 0.7748 | Val Acc: 0.8093 | Val Macro-F1: 0.7383\nEpoch 2/6 | Train Loss: 0.2065 | Train Acc: 0.9180 | Val Acc: 0.9116 | Val Macro-F1: 0.8555\nEpoch 3/6 | Train Loss: 0.2123 | Train Acc: 0.9137 | Val Acc: 0.8884 | Val Macro-F1: 0.8256\nEpoch 4/6 | Train Loss: 0.1344 | Train Acc: 0.9491 | Val Acc: 0.8605 | Val Macro-F1: 0.7926\nEpoch 5/6 | Train Loss: 0.1053 | Train Acc: 0.9620 | Val Acc: 0.9721 | Val Macro-F1: 0.9384\nEpoch 6/6 | Train Loss: 0.1700 | Train Acc: 0.9370 | Val Acc: 0.9907 | Val Macro-F1: 0.9812\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"SAVE_PATH = \"/kaggle/working/xlsr_tamil_final.pt\"\n\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"epochs\": EPOCHS\n    },\n    SAVE_PATH\n)\n\nprint(f\"✅ Model saved after {EPOCHS} epochs at {SAVE_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:27:21.613144Z","iopub.execute_input":"2026-01-28T08:27:21.613619Z","iopub.status.idle":"2026-01-28T08:27:22.862430Z","shell.execute_reply.started":"2026-01-28T08:27:21.613590Z","shell.execute_reply":"2026-01-28T08:27:22.861764Z"}},"outputs":[{"name":"stdout","text":"✅ Model saved after 6 epochs at /kaggle/working/xlsr_tamil_final.pt\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"checkpoint = torch.load(\"/kaggle/working/xlsr_tamil_final.pt\", map_location=device)\n\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\noptimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\nprint(\"✅ Model loaded successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:27:44.540077Z","iopub.execute_input":"2026-01-28T08:27:44.540367Z","iopub.status.idle":"2026-01-28T08:27:45.121851Z","shell.execute_reply.started":"2026-01-28T08:27:44.540341Z","shell.execute_reply":"2026-01-28T08:27:45.121074Z"}},"outputs":[{"name":"stdout","text":"✅ Model loaded successfully\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"TEST_DIR = \"/kaggle/input/depressionintamil/Test-set-tamil/Test-set-tamil\"\n\ntest_files = sorted([\n    os.path.join(TEST_DIR, f)\n    for f in os.listdir(TEST_DIR)\n    if f.endswith(\".wav\")\n])\n\nprint(\"Total test files:\", len(test_files))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:29:36.155539Z","iopub.execute_input":"2026-01-28T08:29:36.156319Z","iopub.status.idle":"2026-01-28T08:29:36.164721Z","shell.execute_reply.started":"2026-01-28T08:29:36.156284Z","shell.execute_reply":"2026-01-28T08:29:36.164088Z"}},"outputs":[{"name":"stdout","text":"Total test files: 160\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"class TestAudioDataset(Dataset):\n    def __init__(self, files):\n        self.files = files\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        audio, _ = librosa.load(self.files[idx], sr=16000)\n        return {\n            \"audio\": audio,\n            \"file\": os.path.basename(self.files[idx])\n        }\ndef test_collate_fn(batch):\n    audios = [item[\"audio\"] for item in batch]\n    files = [item[\"file\"] for item in batch]\n\n    inputs = feature_extractor(\n        audios,\n        sampling_rate=16000,\n        return_tensors=\"pt\",\n        padding=True\n    )\n\n    return {\n        \"input_values\": inputs.input_values,\n        \"files\": files\n    }\ntest_loader = DataLoader(\n    TestAudioDataset(test_files),\n    batch_size=4,\n    shuffle=False,\n    collate_fn=test_collate_fn\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:29:38.603082Z","iopub.execute_input":"2026-01-28T08:29:38.603661Z","iopub.status.idle":"2026-01-28T08:29:38.609786Z","shell.execute_reply.started":"2026-01-28T08:29:38.603631Z","shell.execute_reply":"2026-01-28T08:29:38.609047Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"test_results = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = batch[\"input_values\"].to(device)\n        files = batch[\"files\"]\n\n        logits = model(inputs)\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n        for f, p in zip(files, preds):\n            label = \"Depressed\" if p == 1 else \"Non-depressed\"\n            test_results.append((f, label))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:29:46.555673Z","iopub.execute_input":"2026-01-28T08:29:46.556289Z","iopub.status.idle":"2026-01-28T08:29:56.716963Z","shell.execute_reply.started":"2026-01-28T08:29:46.556259Z","shell.execute_reply":"2026-01-28T08:29:56.716388Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"for i in range(10):\n    print(test_results[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:29:58.700511Z","iopub.execute_input":"2026-01-28T08:29:58.701159Z","iopub.status.idle":"2026-01-28T08:29:58.705086Z","shell.execute_reply.started":"2026-01-28T08:29:58.701128Z","shell.execute_reply":"2026-01-28T08:29:58.704216Z"}},"outputs":[{"name":"stdout","text":"('t1.wav', 'Depressed')\n('t10.wav', 'Non-depressed')\n('t100.wav', 'Depressed')\n('t101.wav', 'Depressed')\n('t102.wav', 'Depressed')\n('t103.wav', 'Depressed')\n('t104.wav', 'Depressed')\n('t105.wav', 'Non-depressed')\n('t106.wav', 'Depressed')\n('t107.wav', 'Non-depressed')\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\n\nsubmission = pd.DataFrame(\n    test_results,\n    columns=[\"file\", \"prediction\"]\n)\n\nsubmission_path = \"/kaggle/working/tamil_test_predictions.csv\"\nsubmission.to_csv(submission_path, index=False)\n\nprint(\"✅ Predictions saved at:\", submission_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}